{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "#from sklearn.utils import shuffle\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Activation, Dense, Flatten, BatchNormalization, MaxPool2D\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Activation, Dense, Flatten, BatchNormalization, MaxPool2D\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data into trai, valid, test dirs\n",
    "os.chdir('C:/Users/aboubakiri.diaw/deploiement recomm/keras/train')\n",
    "if os.path.isdir('train/dog') is False:\n",
    "    os.makedirs('train/dog')\n",
    "    os.makedirs('train/cat')\n",
    "    os.makedirs('valid/dog')\n",
    "    os.makedirs('valid/cat')\n",
    "    os.makedirs('test/dog')\n",
    "    os.makedirs('test/cat')\n",
    "    \n",
    "    for c in random.sample(glob.glob('cat*'), 500):\n",
    "        shutil.move(c, 'train/cat')\n",
    "    for c in random.sample(glob.glob('dog*'), 500): \n",
    "        shutil.move(c, 'train/dog')\n",
    "    for c in random.sample(glob.glob('cat*'), 100):\n",
    "        shutil.move(c, 'valid/cat')\n",
    "    for c in random.sample(glob.glob('dog*'), 100): \n",
    "        shutil.move(c, 'valid/dog') \n",
    "    for c in random.sample(glob.glob('cat*'), 50):\n",
    "        shutil.move(c, 'test/cat') \n",
    "    for c in random.sample(glob.glob('dog*'), 50): \n",
    "        shutil.move(c, 'test/dog')  \n",
    "        \n",
    "os.chdir('../../')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'C:/Users/aboubakiri.diaw/deploiement recomm/Datasets/train'\n",
    "valid_path= 'keras/train/valid'\n",
    "test_path = 'keras/train/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n",
      "Found 0 images belonging to 2 classes.\n",
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocess_input).flow_from_directory(directory=train_path, target_size=(128,128), classes=['cat', 'dog'], batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocess_input).flow_from_directory(directory=valid_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocess_input).flow_from_directory(directory=test_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10, shuffle=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 668 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "X = datagen.flow_from_directory('C:/Users/aboubakiri.diaw/deploiement recomm/keras/train/train',\n",
    "                                    target_size= (128,128),\n",
    "                                    batch_size=15,\n",
    "                                    classes=['cat','df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_batches.n == 1000\n",
    "assert valid_batches.n == 200\n",
    "assert train_batches.num_classes == valid_batches.num_classes == test_batches.num_classes == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will pplot images\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1,10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fe7c1d79101c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotImages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-81640d0b064c>\u001b[0m in \u001b[0;36mplotImages\u001b[1;34m(images_arr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# this function will pplot images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplotImages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(224,224,3)), MaxPool2D(pool_size=(2,2), strides=2), Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'),MaxPool2D(pool_size=(2,2), strides=2), Flatten(), Dense(units=2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 401410    \n",
      "=================================================================\n",
      "Total params: 420,802\n",
      "Trainable params: 420,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"dense_2/Identity:0\", shape=(?, 2), dtype=float32) must be from the same graph as Tensor(\"dense_2_target:0\", shape=(?, ?), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-987f3600d602>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m           \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m           \u001b[0mskip_target_masks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m           masks=self._prepare_output_masks())\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m       \u001b[1;31m# Prepare sample weight modes. List with the same length as model outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[1;34m(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\u001b[0m\n\u001b[0;32m   2002\u001b[0m           metric_results.extend(\n\u001b[0;32m   2003\u001b[0m               self._handle_per_output_metrics(self._per_output_metrics[i],\n\u001b[1;32m-> 2004\u001b[1;33m                                               target, output, output_mask))\n\u001b[0m\u001b[0;32m   2005\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_weighted_and_unweighted_metrics\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreturn_weighted_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m           metric_results.extend(\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[1;34m(self, metrics_dict, y_true, y_pred, mask, weights)\u001b[0m\n\u001b[0;32m   1953\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m         metric_result = training_utils.call_metric_function(\n\u001b[1;32m-> 1955\u001b[1;33m             metric_fn, y_true, y_pred, weights=weights, mask=mask)\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[0mmetric_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetric_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcall_metric_function\u001b[1;34m(metric_fn, y_true, y_pred, weights, mask)\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1156\u001b[0m   \u001b[1;31m# `Mean` metric only takes a single value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m  \u001b[1;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     return distributed_training_utils.call_replica_local_fn(\n\u001b[1;32m--> 196\u001b[1;33m         replica_local_fn, *args, **kwargs)\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\u001b[0m in \u001b[0;36mcall_replica_local_fn\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreplica_local_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplica_local_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m       \u001b[1;34m\"\"\"Updates the state of the metric in a replica-local context.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    583\u001b[0m             [y_true, y_pred], sample_weight)\n\u001b[0;32m    584\u001b[0m     y_pred, y_true = tf_losses_utils.squeeze_or_expand_dimensions(\n\u001b[1;32m--> 585\u001b[1;33m         y_pred, y_true)\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\losses\\util.py\u001b[0m in \u001b[0;36msqueeze_or_expand_dimensions\u001b[1;34m(y_pred, y_true, sample_weight)\u001b[0m\n\u001b[0;32m     71\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred_rank\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_true_rank\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my_pred_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(\n\u001b[1;32m---> 73\u001b[1;33m             y_true, y_pred)\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[1;31m# Use dynamic rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\confusion_matrix.py\u001b[0m in \u001b[0;36mremove_squeezable_dimensions\u001b[1;34m(labels, predictions, expected_rank_diff, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m   \"\"\"\n\u001b[0;32m     59\u001b[0m   with ops.name_scope(name, 'remove_squeezable_dimensions',\n\u001b[1;32m---> 60\u001b[1;33m                       [labels, predictions]):\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6235\u001b[0m       \u001b[1;31m# Specialize based on the knowledge that `_get_graph_from_inputs()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6236\u001b[0m       \u001b[1;31m# ignores `inputs` when building a function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6237\u001b[1;33m       \u001b[0mg_from_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6238\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mg_from_inputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6239\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_from_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5881\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5882\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5883\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5884\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5885\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aboubakiri.diaw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5816\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5817\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[1;32m-> 5818\u001b[1;33m                      (item, original_item))\n\u001b[0m\u001b[0;32m   5819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"dense_2/Identity:0\", shape=(?, 2), dtype=float32) must be from the same graph as Tensor(\"dense_2_target:0\", shape=(?, ?), dtype=float32)."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 33s - loss: 25.4713 - acc: 0.5320 - val_loss: 5.6017 - val_acc: 0.5600\n",
      "Epoch 2/10\n",
      "100/100 - 31s - loss: 3.5407 - acc: 0.6930 - val_loss: 9.8182 - val_acc: 0.5100\n",
      "Epoch 3/10\n",
      "100/100 - 32s - loss: 0.9797 - acc: 0.8400 - val_loss: 1.9900 - val_acc: 0.6650\n",
      "Epoch 4/10\n",
      "100/100 - 32s - loss: 0.3241 - acc: 0.9240 - val_loss: 2.1226 - val_acc: 0.6600\n",
      "Epoch 5/10\n",
      "100/100 - 33s - loss: 0.1043 - acc: 0.9600 - val_loss: 2.1106 - val_acc: 0.6600\n",
      "Epoch 6/10\n",
      "100/100 - 32s - loss: 0.0436 - acc: 0.9870 - val_loss: 2.1188 - val_acc: 0.6500\n",
      "Epoch 7/10\n",
      "100/100 - 34s - loss: 0.0064 - acc: 0.9990 - val_loss: 2.1197 - val_acc: 0.6550\n",
      "Epoch 8/10\n",
      "100/100 - 35s - loss: 0.0024 - acc: 1.0000 - val_loss: 2.1935 - val_acc: 0.6600\n",
      "Epoch 9/10\n",
      "100/100 - 35s - loss: 0.0014 - acc: 1.0000 - val_loss: 2.2266 - val_acc: 0.6700\n",
      "Epoch 10/10\n",
      "100/100 - 35s - loss: 0.0012 - acc: 1.0000 - val_loss: 2.3154 - val_acc: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14bcd937cc8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_imgs, test_labels = next(test_batches)\n",
    "plotImages(imgs)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm= cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    \n",
    "    thresh=cm.max()/2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j,i, cm[i,j], horizontalalignment=\"center\", \n",
    "        color=\"white\" if cm[i,j]> thresh else \"black\")\n",
    "                 \n",
    "                 \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix, without normalization\n",
      "[[36 14]\n",
      " [20 30]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_plot_labels = ['cat', 'dog']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buil Fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.models import load_model\n",
    "vgg16_weights ='C:/Users/aboubakiri.diaw/deploiement recomm/keras/vgg16.h5'\n",
    "\n",
    "vgg16_model = VGG16(weights=vgg16_weights)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.training.Model"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\aboubakiri.diaw\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 321s - loss: 0.2465 - acc: 0.8990 - val_loss: 0.2051 - val_acc: 0.9000\n",
      "Epoch 2/5\n",
      "100/100 - 298s - loss: 0.1063 - acc: 0.9650 - val_loss: 0.1729 - val_acc: 0.9050\n",
      "Epoch 3/5\n",
      "100/100 - 301s - loss: 0.0733 - acc: 0.9730 - val_loss: 0.1590 - val_acc: 0.9300\n",
      "Epoch 4/5\n",
      "100/100 - 301s - loss: 0.0556 - acc: 0.9820 - val_loss: 0.1548 - val_acc: 0.9250\n",
      "Epoch 5/5\n",
      "100/100 - 300s - loss: 0.0413 - acc: 0.9890 - val_loss: 0.1402 - val_acc: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c716b29c8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix, without normalization\n",
      "[[48  2]\n",
      " [ 7 43]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_plot_labels = ['cat', 'dog']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
